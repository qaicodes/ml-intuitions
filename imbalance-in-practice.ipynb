{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Imbalanced Data\n",
    "\n",
    "The Core Issue: In an ideal world, your dataset for a classification problem would have a roughly equal distribution of examples for each class you're trying to predict. An imbalanced dataset means that one or more classes (the \"majority\" classes) have significantly more examples than others (the \"minority\" classes).\n",
    "\n",
    "Why It's a Problem: Machine learning algorithms inherently like to find patterns. When there's a huge imbalance, the algorithm may \"learn\" to simply favor the majority class to boost its accuracy score. This leads it to neglect the minority class, even though that class might be the one we care most about (e.g., detecting fraudulent transactions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Learning\n",
    "\n",
    "Let's work with a dataset to make this concrete. We'll use a hypothetical example for credit card fraud detection. Download dataset from [Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?resource=download)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditcard = pd.read_csv(\"creditcard.csv\")\n",
    "creditcard.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Without Addressing Imbalance (My Turn)\n",
    "\n",
    "Let's do this part initially to show the problem:\n",
    "\n",
    "* We'll split the data into features (X) and target (y).\n",
    "* We'll train a simple model (like Logistic Regression) on this data.\n",
    "* We'll evaluate the model. Notice the deceptively high accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check the value counts for each target label to get an idea of what problem we are delaing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9991994606893064\n",
      "Precision: 0.8414634146341463\n",
      "Recall: 0.6106194690265486\n",
      "F1-Score: 0.7076923076923077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "\n",
    "# Split into features and target\n",
    "X = creditcard.drop('Class', axis=1) \n",
    "y = creditcard['Class']\n",
    "\n",
    "# Split into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) \n",
    "\n",
    "# Instantiate a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Fit the model without handling imbalance\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('F1-Score:', f1_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's analyze these results in the context of our imbalanced fraud detection task:\n",
    "\n",
    "* __Accuracy (0.999199)__:  Seems fantastic, right? But this is highly misleading in scenarios with imbalanced classes. The model could be mostly predicting the majority class (normal transactions) and still get this score.\n",
    "\n",
    "* __Precision (0.841463)__: A respectable precision score tells us that approximately 84% of transactions the model flagged as fraudulent are actually fraudulent. Note this does depend on the prevalence of fraud in the test set.\n",
    "\n",
    "* __Recall (0.610619)__:  This is where it gets worrisome. Recall tells us how many of the actual fraudulent transactions our model successfully detected. In this case, we're missing nearly 40% of the fraudulent cases!  This might lead to significant financial losses.\n",
    "\n",
    "* __F1-Score (0.707692)__: The F1-score is a harmonic mean between precision and recall, giving a balanced view.  A higher F1-score generally indicates a better overall model performance in imbalanced situations.\n",
    "\n",
    "__Key Takeaway__\n",
    "\n",
    "* The issue with imbalanced data is clear. It's easy for a model to get \"lazy\" and prioritize the majority class, often sacrificing its ability to accurately detect the minority class, even when that minority class (fraudulent transactions) is the one we actually care about the most.\n",
    "* Precision is majorly not a good metric for such problem where minority class is costly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __What Now?__\n",
    "\n",
    "This analysis emphasizes the need to address the class imbalance directly. Let's move on to try the following techniques:\n",
    "\n",
    "Oversampling:\n",
    "\n",
    "* Use RandomOverSampler from the imblearn library to generate more synthetic instances of fraudulent transactions.\n",
    "Undersampling:\n",
    "\n",
    "* Use RandomUnderSampler to reduce the number of normal transactions.\n",
    "\n",
    "* Employ SMOTE (Synthetic Minority Oversampling Technique) for a more sophisticated approach to generating synthetic minority examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9769809836802337\n",
      "Precision: 0.06286043829296424\n",
      "Recall: 0.8861788617886179\n",
      "F1-Score: 0.11739364566505116\n"
     ]
    }
   ],
   "source": [
    "# Split into features and target\n",
    "X = creditcard.drop(\"Class\", axis=1)\n",
    "y = creditcard[\"Class\"]\n",
    "\n",
    "# Define the oversampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Oversample and split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Instantiate a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Fit the model without handling imbalance\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Let's analyze the changes caused by oversampling:__\n",
    "\n",
    "__Changes__\n",
    "* __Recall__: A substantial increase! Our model catches roughly 89% of actual fraudulent transactions compared to the previous 61%. This is exactly what we wanted.\n",
    "* __Precision__: A significant decrease. Now, a greater proportion of transactions flagged as fraudulent are actually false alarms.\n",
    "* __Accuracy:__ Deceptive decrease. Oversampling has made the problem harder for the classifier in terms of the overall dataset as it's no longer dominated by one class.\n",
    "* __F1-score__: This also decreased, reflecting the trade-off between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __The Trade-Off__\n",
    "\n",
    "This illustrates a common theme with imbalanced datasets: it's a balancing act. By oversampling, we've:\n",
    "\n",
    "* __Improved Sensitivity:__ The model identifies fraud better, important for our business case.\n",
    "* __Lost Specificity:__ More normal transactions are falsely flagged, possibly a nuisance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __What Does This Mean?__\n",
    "\n",
    "There's no universally \"correct\" answer. What's better depends on our priorities:\n",
    "\n",
    "* __False Negatives are Very Costly:__ In a high-risk fraud scenario, we might accept having more false positives if it prevents large financial losses due to missed fraud.\n",
    "\n",
    "* __False Positives Create Customer Friction:__ If unnecessarily blocking customer transactions is bad for the business model, we might favor more precision-focused solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
